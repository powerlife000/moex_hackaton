{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87263167",
   "metadata": {},
   "source": [
    "# Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a65e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python310\\lib\\site-packages\\mpl_finance.py:16: DeprecationWarning: \n",
      "\n",
      "  =================================================================\n",
      "\n",
      "   WARNING: `mpl_finance` is deprecated:\n",
      "\n",
      "    Please use `mplfinance` instead (no hyphen, no underscore).\n",
      "\n",
      "    To install: `pip install --upgrade mplfinance` \n",
      "\n",
      "   For more information, see: https://pypi.org/project/mplfinance/\n",
      "\n",
      "  =================================================================\n",
      "\n",
      "  __warnings.warn('\\n\\n  ================================================================='+\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.dates import MONDAY, DateFormatter, DayLocator, WeekdayLocator\n",
    "\n",
    "from mpl_finance import candlestick_ohlc  #  pip install mpl-finance\n",
    "import mplfinance as mpf\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import talib\n",
    "import pickle\n",
    "import yfinance as yf\n",
    "# from IPython.display import clear_output\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5dc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "from tensorflow.keras.models import load_model\n",
    "from array import *\n",
    "import os.path\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, auc, accuracy_score, roc_auc_score,f1_score,log_loss,\\\n",
    "classification_report, roc_curve\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from sys import argv #Module for receiving parameters from the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaefdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import argparse\n",
    "import psycopg2\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4cec18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from moexalgo import Market, Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1db827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55be9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec37376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e665dd",
   "metadata": {},
   "source": [
    "# Импортируем модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8749ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b255b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function _signal.default_int_handler(signalnum, frame, /)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def signal_handler(signal, frame):\n",
    "    print(\"\\nprogram exiting gracefully\")\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f82ab696",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73d95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Системные модули\n",
    "from DB_module import DB\n",
    "from Config_module import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5bb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "691bcb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Модули генерации датасета\n",
    "from date_filter import date_filter #Фильтрация данных по датам date_filter(quotes, filter_data_timezone, filter_data_start, filter_data_end)\n",
    "from show_quotes import show_quotes #Смотрим исходные данные show_quotes(quotes)\n",
    "from get_extrems import get_extrems #Получаем экстремумы get_extrems(dataset, delete_not_marking_data, count_points = 6)\n",
    "from show_quotes_with_trends import show_quotes_with_trends #Просмотр результатов разметки show_quotes_with_trends(quotes_with_extrems, show = False)\n",
    "from quotes_with_Y import quotes_with_Y#Разметка Y quotes_with_Y(quotes_with_extrems, extr_bar_count, Y_shift)\n",
    "from get_indicators import get_indicators #Получение индикаторов для котировок get_indicators(df, prefix = ':1d')\n",
    "from get_stoch_indicators import get_stoch_indicators#Обработка стохастика над индикаторами get_stoch_indicators(df, prefix = ':1d')\n",
    "from get_stoch_logic_data import get_stoch_logic_data#Генерация логического датасета над датасетом стохастика get_stoch_logic_data(df, prefix = ':1d')\n",
    "from norm_num_df import norm_num_df# Генерация нормализованного числового датасета norm_num_df(df, prefix = ':1d')\n",
    "from waves_dataset import waves_dataset#Генерация датасета по экстремумам waves_dataset(df, prefix = ':1d')\n",
    "from logic_dataset import logic_dataset#Генерация датасета на основании логических конструкций logic_dataset(df, prefix = ':1d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910f9c4",
   "metadata": {},
   "source": [
    "# Параметры генерируемого датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02c9fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_params_from_config_file = True #Загрузка параметров из файла\n",
    "load_params_from_command_line = False #Загрузка параметров из командной строки\n",
    "args = None\n",
    "\n",
    "try:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    _ = parser.add_argument('--config_file', dest='config_file', action='store_true', help='Load config from file')\n",
    "    _ = parser.add_argument('--config_path', help='Path to config file: /app/cfg.json')\n",
    "    _ = parser.add_argument('--cmd_config', dest='cmd_config', action='store_true', help='Load config from cmd line')\n",
    "    _ = parser.add_argument('--task_id')\n",
    "    _ = parser.add_argument('--tickers')\n",
    "    _ = parser.add_argument('--scaler_path')\n",
    "    _ = parser.add_argument('--neural_path')\n",
    "    _ = parser.add_argument('--timeframe')\n",
    "    _ = parser.add_argument('--count_points')\n",
    "    _ = parser.add_argument('--extr_bar_count')\n",
    "    _ = parser.add_argument('--max_unmark')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if args.config_file:\n",
    "        load_params_from_config_file = True\n",
    "        load_params_from_command_line = False\n",
    "    \n",
    "    if args.cmd_config:\n",
    "            load_params_from_config_file = False\n",
    "            load_params_from_command_line = True\n",
    "except:\n",
    "    print(\"Ошибка парсинга параметров из командной строки\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0472e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_params_from_config_file:\n",
    "    #Если есть параметры командной строки\n",
    "    if args:\n",
    "        #Если указан путь к конфигу\n",
    "        if args.config_path:\n",
    "            with open(config_path, 'r', encoding='utf_8') as cfg:\n",
    "                temp_data=cfg.read()\n",
    "        else:\n",
    "            with open('app/configs/10m/calc_signals.json', 'r', encoding='utf_8') as cfg:\n",
    "                temp_data=cfg.read()\n",
    "\n",
    "    # parse file`\n",
    "    config = json.loads(temp_data)\n",
    "    \n",
    "    task_id = str(config['task_id'])\n",
    "    #Список тикеров для генерации сигналов\n",
    "    tickers = config['tickers']\n",
    "    #Путь для сохранения скалера\n",
    "    scaler_path = config['scaler_path'] #Путь должен быть без чёрточки в конце\n",
    "    #Путь для сохранения нейронных сетей\n",
    "    neural_path = config['neural_path'] #Путь должен быть без чёрточки в конце\n",
    "    interval = config['timeframe']\n",
    "    count_points = config['count_points'] #Параметр разметки экстремумов\n",
    "    extr_bar_count = config['extr_bar_count'] #Сколько баров размечаем для генерации сигналов\n",
    "    #Максимальное количество конечных баров волны в %, которые не размечаем\n",
    "    max_unmark = config['max_unmark']\n",
    "\n",
    "#Число дней в датасете\n",
    "count_datys = config['count_datys']\n",
    "    \n",
    "if load_params_from_command_line:\n",
    "    task_id = str(args.task_id)\n",
    "    tickers = args.tickers.replace(']',\"\").replace('[',\"\").replace('\"',\"\").replace(\"'\",\"\").split(\",\")\n",
    "    scaler_path = str(args.scaler_path)\n",
    "    neural_path = str(args.neural_path) \n",
    "    interval = str(args.timeframe)\n",
    "    count_points = int(args.count_points) \n",
    "    extr_bar_count = int(args.extr_bar_count) \n",
    "    max_unmark = float(args.max_unmark) \n",
    "\n",
    "Y_shift = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f39a76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Смещение категориальных признаков разметки\n",
    "Y_shift = 1\n",
    "\n",
    "#Флаг необходимости формирования трендовых признаков\n",
    "lag_flag = True\n",
    "\n",
    "#Число баров, которые мы кладём в датасет для формирования признаков трендовости\n",
    "#Число включает начальный бар без лага, то есть из 6: 1 - начальный + 5 лаговые\n",
    "lag_count = 0\n",
    "\n",
    "#Флаг необходимости масштабирования данных\n",
    "scale_flag = True\n",
    "\n",
    "#По какому количеству открытых позиций нужно проходить?\n",
    "open_positions_count = 5\n",
    "\n",
    "#Флаг необходимости удаления не размеченных данных\n",
    "delete_not_marking_data = False\n",
    "\n",
    "#Максимальное количество конечных баров волны в %, которые не размечаем\n",
    "max_unmark = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48c9c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_to_png(graph):\n",
    "    buffer = io.BytesIO()\n",
    "    graph.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    image_png = buffer.getvalue()\n",
    "    buffer.close()\n",
    "    graphic = base64.b64encode(image_png)\n",
    "    graphic = graphic.decode('utf-8')\n",
    "    graph.close()\n",
    "\n",
    "    return graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e55ad5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main (ticker, start_date, end_date):\n",
    "    \n",
    "    #КОТИРОВКИ!\n",
    "    #Получаем дневные данные\n",
    "#     quotes_1d_temp=yf.Ticker(ticker)\n",
    "#     quotes_1d=quotes_1d_temp.history(\n",
    "#         interval = \"1d\",# valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "#         period=\"max\"\n",
    "#     ) #  1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "#     quotes_1d = quotes_1d.dropna()\n",
    "#     quotes_1d.index = quotes_1d.index.tz_localize(None)\n",
    "#     quotes_1d.sort_index(ascending=True, inplace = True)\n",
    "#     quotes_1d.index.name = \"Datetime\"\n",
    "\n",
    "    # Акции\n",
    "    quotes_temp = Ticker(ticker)\n",
    "    # Свечи по акциям за период\n",
    "    quotes_1d = quotes_temp.candles(date = start_date, till_date = end_date, period=interval)\n",
    "    quotes_1d.head()\n",
    "    \n",
    "    quotes_1d.rename(\n",
    "        columns = {\n",
    "            'begin' : 'Datetime',\n",
    "            'open' : 'Open',\n",
    "            'close' : 'Close',\n",
    "            'high' : 'High',\n",
    "            'low' : 'Low',\n",
    "            'volume' : 'Volume'\n",
    "        }, inplace = True\n",
    "    )\n",
    "    quotes_1d.index = quotes_1d['Datetime']\n",
    "    quotes_1d.sort_index(ascending=True, inplace = True)\n",
    "\n",
    "#     #Фильтруем данные\n",
    "#     if data_filter_flag:\n",
    "#         quotes_1d = date_filter(quotes_1d, filter_data_timezone, filter_data_start, filter_data_end)\n",
    "\n",
    "    #Получаем экстремумы по дневному графику\n",
    "    print('Получаем экстремумы по дневному графику')\n",
    "    quotes_1d_with_extrems = get_extrems(quotes_1d, delete_not_marking_data, count_points = count_points)\n",
    "\n",
    "    #Размечаем Y по дневному графику\n",
    "    quotes_1d_with_Y = quotes_with_Y(quotes_1d_with_extrems, extr_bar_count, Y_shift, max_unmark = max_unmark)\n",
    "\n",
    "    #Очищаем не размеченные данные\n",
    "    quotes_1d_with_Y = quotes_1d_with_Y.dropna(subset = ['Y'])\n",
    "\n",
    "    #Получаем данные индикаторов котировок дневного датафрейма\n",
    "    quotes_1d_indicators = get_indicators(quotes_1d_with_Y, prefix = ':5m')\n",
    "\n",
    "    #Получаем stoch датасет для котировок дневного таймфрейма\n",
    "    stoch_quotes_1d_dataset = get_stoch_indicators(quotes_1d_indicators, prefix = ':5m')\n",
    "\n",
    "    #Получаем датасет логики над стохастиком для котировок дневного таймфрейма\n",
    "    stoch_logic_quotes_1d_dataset = get_stoch_logic_data(stoch_quotes_1d_dataset, prefix = ':5m')\n",
    "    \n",
    "    #Получаем нормализованный числовой датасет для котировок дневного таймфрейма\n",
    "    norm_num_dataset_quotes_1d = norm_num_df(quotes_1d_indicators, prefix = ':5m')\n",
    "\n",
    "    #Свечной анализ\n",
    "    cdl_dataset_quotes_1d = quotes_1d.ta.cdl_pattern(name=\"all\")\n",
    "\n",
    "    #Датасет волн\n",
    "    waves_dataset_quotes_1d =  waves_dataset(quotes_1d_indicators, prefix = ':5m')\n",
    "\n",
    "    #Логический датасет\n",
    "    logic_dataset_quotes_1d =  logic_dataset(quotes_1d_indicators, prefix = ':5m')\n",
    "    \n",
    "    #Собираем датасеты\n",
    "    num_logic_df = pd.DataFrame()\n",
    "    \n",
    "    #Формируем индекс по древным котировкам\n",
    "    num_logic_df.index = quotes_1d.index\n",
    "    \n",
    "    #Инициализируем поля\n",
    "    num_logic_df['Close'] = quotes_1d_with_Y['Close']\n",
    "    num_logic_df['Y'] = quotes_1d_with_Y['Y']\n",
    "    \n",
    "    \n",
    "    #Джойним датасеты\n",
    "    num_logic_df = num_logic_df.join(norm_num_dataset_quotes_1d, lsuffix='_left_num_qout_5m', rsuffix='_right_num_qout_5m')#Нормализованные дневные котировки\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(waves_dataset_quotes_1d, lsuffix='_left_num_qout_5m', rsuffix='_right_num_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(cdl_dataset_quotes_1d, lsuffix='_left_num_qout_5m', rsuffix='_right_num_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(stoch_quotes_1d_dataset, lsuffix='_left_stoch_qout_5m', rsuffix='_right_stoch_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(stoch_logic_quotes_1d_dataset, lsuffix='_left_stoch_qout_5m', rsuffix='_right_stoch_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(logic_dataset_quotes_1d, lsuffix='_left_logic_qout_5m', rsuffix='_right_logic_qout_5m')\n",
    "    \n",
    "    \n",
    "    #Заполняем пустые ячейки предыдущими значениями\n",
    "    num_logic_df = num_logic_df.fillna(method=\"ffill\")\n",
    "     \n",
    "    #Добавляем лаги\n",
    "    #num_df\n",
    "    columns = num_logic_df.columns.values   \n",
    "    for col in columns:\n",
    "        if col not in ['Close', 'Y']:\n",
    "            try:\n",
    "                for i in range(1,lag_count):\n",
    "                    num_logic_df[col+'shift_'+str(i)] = num_logic_df[col].copy(deep = True).shift(i)\n",
    "            except:\n",
    "                #print(\"Ошибка добавления лага в колонке: \", col)\n",
    "                pass\n",
    "    \n",
    "    \n",
    "    #Чистим от пустых значений\n",
    "    num_logic_df = num_logic_df.dropna()\n",
    "    \n",
    "    #Конвертируем индексы\n",
    "    num_logic_df.index = num_logic_df.index.astype(int)\n",
    "    \n",
    "    return num_logic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3dda208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подготовка данных\n",
    "def prepade_df(df, dataset):\n",
    "    \n",
    "    init_data_train = df\n",
    "\n",
    "    # Устанавливаем размерность датасетов\n",
    "    n_train = init_data_train.shape[0]\n",
    "    p_train = init_data_train.shape[1]\n",
    "    print(\"Число факторов: \", p_train)\n",
    "    # Формируем данные в numpy-массив\n",
    "    init_data_train = init_data_train.values\n",
    "    # Подготовка данных для обучения и тестирования (проверки)\n",
    "    print(\"Подготавливаем выборки\")\n",
    "    train_start = 0\n",
    "    train_end = n_train\n",
    "    data_train = init_data_train[np.arange(train_start, train_end), :]\n",
    "    #Выбор данных\n",
    "    print(\"Выбираем данные\")\n",
    "    trainX = data_train[:, 2:]\n",
    "    trainY = data_train[:, 1]\n",
    "    train_quotes_close = data_train[:, 0]\n",
    "\n",
    "    #Изменяем размерность массива, для обеспечения возможности масштабирования Y\n",
    "    trainY = trainY.reshape(-1, 1)\n",
    "    train_quotes_close = train_quotes_close.reshape(-1, 1)\n",
    "    \n",
    "    if scale_flag:\n",
    "        #Загружаем скалер\n",
    "        x_scaler = joblib.load('./'+scaler_path+'/scaler_'+dataset+'.save')\n",
    "        \n",
    "        trainX = x_scaler.transform(trainX)\n",
    "        \n",
    "    #Изменяем размерность массива Х, для рекурентной нейросети\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "        \n",
    "    return trainX, trainY, train_quotes_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5318c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Расчёт на основании модели\n",
    "def calc_signals(model, trainX, trainY, train_quotes_close):\n",
    "    \n",
    "    print(\"Предсказываем результат\")\n",
    "    predict_trainY = model.predict(trainX, verbose = 1)\n",
    "\n",
    "    #Преобразовываем выходные сигналы тренировочной выборки\n",
    "\n",
    "    result_predict_trainY = []\n",
    "\n",
    "    for predict in predict_trainY:\n",
    "        result_predict_trainY.append(np.argmax(predict))\n",
    "\n",
    "    result_predict_trainY = np.array(result_predict_trainY)\n",
    "    \n",
    "    np_result_Y = np.rint(result_predict_trainY)\n",
    "\n",
    "        #Расчёт трендов по разметке\n",
    "    last_train_signal = 2\n",
    "    trends_origin = array('f', []) #Массив ожидаемых данных по тренду\n",
    "    for i in range(trainY.shape[0]):\n",
    "        if trainY[i] != last_train_signal and (trainY[i] == 2 or trainY[i] == 0):\n",
    "            last_train_signal = trainY[i]\n",
    "        trends_origin.insert(i,last_train_signal)\n",
    "    \n",
    "    #Расчёт трендов для расчётных значений\n",
    "    last_test_signal = 2\n",
    "    trends_predict = array('f', []) #Массив ожидаемых данных по тренду\n",
    "    for i in range(len(np_result_Y)):\n",
    "        if np_result_Y[i] != last_test_signal and (np_result_Y[i] == 2 or np_result_Y[i] == 0):\n",
    "            last_test_signal = np_result_Y[i]\n",
    "        trends_predict.insert(i,last_test_signal)\n",
    "    \n",
    "    trends_origin = np.asarray(trends_origin).astype(int)\n",
    "    trends_predict = np.asarray(trends_predict).astype(int)\n",
    "    \n",
    "    \n",
    "#     print(trends_origin)\n",
    "#     print(trends_predict)\n",
    "        \n",
    "    f1_metric = f1_score(trends_origin, trends_predict, pos_label=2)\n",
    "    \n",
    "#     print(f1_metric)\n",
    "\n",
    "    return np_result_Y, trends_predict, trends_origin, f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc2aff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_count = 0\n",
    "def get_new_ticker():\n",
    "    global ticker_count\n",
    "    ticker = tickers[ticker_count]\n",
    "    \n",
    "    ticker_count += 1\n",
    "    \n",
    "    if ticker_count >=  len(tickers):\n",
    "        ticker_count = 0\n",
    "    \n",
    "    return ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71657ac",
   "metadata": {},
   "source": [
    "# Загружаем нейронные сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2bbb3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем инвестиционные нейронные сети \"neurals_tech_for_investing_signals\"\n",
    "model_num_logic = load_model('./'+neural_path+'/ansamble_num_logic_1d_1w_v1.h5', compile=False)\n",
    "model_num_logic.compile() #Paste it here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357be5b",
   "metadata": {},
   "source": [
    "# Делаем запись о задаче по запуску сервиса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4f0fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Соединение с БД\n",
    "def connect():\n",
    "    return psycopg2.connect(\n",
    "        host=global_config.db_host,\n",
    "        database=global_config.db_database,\n",
    "        user=global_config.db_user,\n",
    "        password=global_config.db_password\n",
    "    )\n",
    "conn = connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed22706",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Обновляем данные по задаче\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[38;5;241m.\u001b[39mclosed \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      3\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connect()\n\u001b[0;32m      4\u001b[0m cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "#Обновляем данные по задаче\n",
    "if conn.closed == 1:\n",
    "    conn = connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql = \"\"\" UPDATE calc_signals_tasks\n",
    "            SET task_status = %s\n",
    "            WHERE id = %s\"\"\"\n",
    "try:\n",
    "    cur.execute(sql, ('done', task_id))\n",
    "except Exception as e:\n",
    "    print(\"Ошибка записи информации о закрытии задачи в БД: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9acb5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f28dc8",
   "metadata": {},
   "source": [
    "# Загружаем новый тикер и обрабатываем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58154b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем обработку нового тикера:  ABIO 2023-11-26 18:09:49.344556\n",
      "Получаем датасеты\n",
      "Получаем экстремумы по дневному графику\n",
      "Общее число данных графика для обработки:  2322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adimin\\AppData\\Local\\Temp\\ipykernel_145724\\2120635346.py:114: FutureWarning: The behavior of .astype from datetime64[ns] to int32 is deprecated. In a future version, this astype will return exactly the specified dtype instead of int64, and will raise if that conversion overflows.\n",
      "  num_logic_df.index = num_logic_df.index.astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобрабатываем датасеты\n",
      "Число факторов:  2661\n",
      "Подготавливаем выборки\n",
      "Выбираем данные\n",
      "Предсказываем результат\n",
      "69/69 [==============================] - 1s 10ms/step\n",
      "Записываем сигнал в базу данных\n",
      "4 0.0\n",
      "Обновляем результаты\n",
      "Тикер обработан и записан в базу:  ABIO 2023-11-26 18:10:17.369456\n",
      "Время обработки:  ABIO 28.0249\n",
      "Начинаем обработку нового тикера:  SBER 2023-11-26 18:10:17.369456\n",
      "Получаем датасеты\n",
      "Получаем экстремумы по дневному графику\n",
      "Общее число данных графика для обработки:  3569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adimin\\AppData\\Local\\Temp\\ipykernel_145724\\2120635346.py:114: FutureWarning: The behavior of .astype from datetime64[ns] to int32 is deprecated. In a future version, this astype will return exactly the specified dtype instead of int64, and will raise if that conversion overflows.\n",
      "  num_logic_df.index = num_logic_df.index.astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобрабатываем датасеты\n",
      "Число факторов:  2661\n",
      "Подготавливаем выборки\n",
      "Выбираем данные\n",
      "Предсказываем результат\n",
      "108/108 [==============================] - 1s 11ms/step\n",
      "Записываем сигнал в базу данных\n",
      "5 0.0\n",
      "Обновляем результаты\n",
      "Тикер обработан и записан в базу:  SBER 2023-11-26 18:11:02.617716\n",
      "Время обработки:  SBER 45.24826\n",
      "Начинаем обработку нового тикера:  ABIO 2023-11-26 18:11:02.618717\n",
      "Получаем датасеты\n",
      "Получаем экстремумы по дневному графику\n",
      "Общее число данных графика для обработки:  2322\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        #Получаем новый тикер для обработки\n",
    "        try:\n",
    "            ticker = get_new_ticker()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Keyboard interrupt exception caught\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Ошибка получения тикера из БД: \", datetime.datetime.now(), ' e: ', e)\n",
    "            continue\n",
    "            \n",
    "        #Опеределяем начальную и конечную даты для фильтрации\n",
    "        start_date = datetime.datetime.today() - datetime.timedelta(days=count_datys)\n",
    "        end_date = datetime.datetime.today() - datetime.timedelta(days=0)\n",
    "\n",
    "        #Преобразовываем в стринги\n",
    "        start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "        end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "        if ticker != None:\n",
    "            start_date_1 = datetime.datetime.now()\n",
    "            print(\"Начинаем обработку нового тикера: \", ticker, start_date_1)\n",
    "            \n",
    "            #Получаем датасеты\n",
    "            try:\n",
    "                print(\"Получаем датасеты\")\n",
    "                temp = main(ticker, start_date, end_date)\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Keyboard interrupt exception caught\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Ошибка получения датасетов: \", datetime.datetime.now(), ' e: ', e)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                #Предобрабатываем датасеты\n",
    "                print(\"Предобрабатываем датасеты\")\n",
    "                num_logic_for_neurals = prepade_df(temp, 'num_logic_1d_1w')\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Keyboard interrupt exception caught\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Ошибка предобработки датасетов: \", datetime.datetime.now(), ' e: ', e)\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            try:\n",
    "                #Расчёт сигналов\n",
    "                ansamble_signals_temp = calc_signals(model_num_logic, num_logic_for_neurals[0], num_logic_for_neurals[1], num_logic_for_neurals[2])\n",
    "                ansamble_signals = ansamble_signals_temp[0]\n",
    "\n",
    "                f1_metric = ansamble_signals_temp[3]\n",
    "\n",
    "                f1_metric = ansamble_signals_temp[3]\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Keyboard interrupt exception caught\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Ошибка расчёта сигналов: \", datetime.datetime.now(), ' e: ', e)\n",
    "                continue\n",
    "                \n",
    "                \n",
    "                \n",
    "            try:\n",
    "                #Записываем сигнал в базу данных\n",
    "                print(\"Записываем сигнал в базу данных\")\n",
    "                stoch_signal = None\n",
    "                last_stoch_signal = None\n",
    "                num_signal = None\n",
    "                last_num_signal = None\n",
    "                logic_signal = None\n",
    "                last_logic_signal = None\n",
    "#                                     ansamble_signal = ansamble_signals[ansamble_signals.shape[0]-1]\n",
    "#                                     last_ansamble_signal = ansamble_signals[ansamble_signals.shape[0]-1]\n",
    "\n",
    "                df_ansamble = pd.DataFrame(ansamble_signals)\n",
    "\n",
    "                #ansamble_signal = df_ansamble[df_ansamble[0] != 1].iloc[-1].values[0]\n",
    "                #last_ansamble_signal = df_ansamble[df_ansamble[0] != 1].iloc[-2].values[0]\n",
    "                last_ansamble_signal = 1\n",
    "\n",
    "                ansamble_signal_length = df_ansamble.shape[0]-1\n",
    "\n",
    "#                                         ansamble_signal_position = ansamble_signal_length - df_ansamble[df_ansamble[0] != 1].iloc[-1].name\n",
    "\n",
    "                current_signal = None\n",
    "                count_pos = 0\n",
    "\n",
    "                for i,row in  df_ansamble.iterrows():\n",
    "\n",
    "                    if ((row[0] == 0.0) | (row[0] == 2.0)) & (row[0] != current_signal):\n",
    "                        current_signal = row[0]\n",
    "                        count_pos = 0\n",
    "\n",
    "                    count_pos += 1\n",
    "\n",
    "                ansamble_signal_position = count_pos\n",
    "                ansamble_signal = current_signal\n",
    "                \n",
    "                print(ansamble_signal_position, ansamble_signal)\n",
    "\n",
    "                #Проверяем наличие записей в базе\n",
    "                now = datetime.datetime.now() # current date and time\n",
    "                date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                \n",
    "                #Пытаемся записать результаты\n",
    "                try:\n",
    "                    if conn.closed == 1:\n",
    "                        conn = connect()\n",
    "                    #Проверяем наличие записи\n",
    "                    cur = conn.cursor()\n",
    "                    cur.execute(\"SELECT * FROM public.edu_neural_results WHERE task_id  = %s;\", (task_id,))\n",
    "                    results = cur.fetchall()\n",
    "                    cur.close()\n",
    "                    \n",
    "                    if conn.closed == 1:\n",
    "                        conn = connect()\n",
    "                    #Проверяем наличие записи\n",
    "                    cur = conn.cursor()\n",
    "                    \n",
    "                    if len(results) == 0:\n",
    "                        print(\"Записываем результаты\")\n",
    "                        #Записи о результатах в БД нет, записываем новый результат\n",
    "                        cur.execute(\n",
    "                            \"\"\"\n",
    "                            INSERT INTO public.cals_signals_results\n",
    "                            (\n",
    "                                task_id,\n",
    "                                ticker,\n",
    "                                signal,\n",
    "                                signal_position,\n",
    "                            )\n",
    "                            VALUES (%s, %s, %s, %s);\n",
    "                            \"\"\",\n",
    "                            (\n",
    "                                task_id, \n",
    "                                ticker, \n",
    "                                ansamble_signal, \n",
    "                                ansamble_signal_position\n",
    "                            )\n",
    "                        )\n",
    "                    else:\n",
    "                        #Обновляем запись\n",
    "                        print(\"Обновляем результаты\")\n",
    "                        sql = \"\"\" UPDATE public.cals_signals_results\n",
    "                                    SET \n",
    "                                    signal = %s,\n",
    "                                    signal_position = %s\n",
    "                                    WHERE task_id = %s AND ticker = %s \"\"\"\n",
    "                        cur.execute(sql, (\n",
    "                                ansamble_signal, \n",
    "                                ansamble_signal_position,\n",
    "                                task_id,\n",
    "                                ticker\n",
    "                            ))\n",
    "                    cur.close()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"Ошибка записи результатов в БД: \", e)\n",
    "                    conn = connect()\n",
    "\n",
    "                end_date = datetime.datetime.now()\n",
    "                print(\"Тикер обработан и записан в базу: \", ticker, end_date)\n",
    "                print(\"Время обработки: \", ticker, (end_date-start_date_1).total_seconds())\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Keyboard interrupt exception caught\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Ошибка записи сигнала в БД: \", datetime.datetime.now(), ' e: ', e)\n",
    "                continue\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Keyboard interrupt exception caught\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Не учтённая ошибка: \", datetime.datetime.now(), ' e: ', e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f45e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
