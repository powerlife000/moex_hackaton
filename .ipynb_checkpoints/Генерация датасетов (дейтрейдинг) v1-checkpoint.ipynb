{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c828fe",
   "metadata": {},
   "source": [
    "# Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e73a17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import pandas as pd\n",
    "from matplotlib.dates import MONDAY, DateFormatter, DayLocator, WeekdayLocator\n",
    "\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from moexalgo import Market, Ticker\n",
    "import io\n",
    "from PIL import Image\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b359da",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2915445",
   "metadata": {},
   "source": [
    "# Импортируем модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10132406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8899a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b95189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Системные модули\n",
    "from DB_module import DB\n",
    "from Config_module import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c517873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Модули генерации датасета\n",
    "from date_filter import date_filter #Фильтрация данных по датам date_filter(quotes, filter_data_timezone, filter_data_start, filter_data_end)\n",
    "from show_quotes import show_quotes #Смотрим исходные данные show_quotes(quotes)\n",
    "from get_extrems import get_extrems #Получаем экстремумы get_extrems(dataset, delete_not_marking_data, count_points = 6)\n",
    "from show_quotes_with_trends import show_quotes_with_trends #Просмотр результатов разметки show_quotes_with_trends(quotes_with_extrems, show = False)\n",
    "from quotes_with_Y import quotes_with_Y#Разметка Y quotes_with_Y(quotes_with_extrems, extr_bar_count, Y_shift)\n",
    "from get_indicators import get_indicators #Получение индикаторов для котировок get_indicators(df, prefix = ':1d')\n",
    "from get_stoch_indicators import get_stoch_indicators#Обработка стохастика над индикаторами get_stoch_indicators(df, prefix = ':1d')\n",
    "from get_stoch_logic_data import get_stoch_logic_data#Генерация логического датасета над датасетом стохастика get_stoch_logic_data(df, prefix = ':1d')\n",
    "from norm_num_df import norm_num_df# Генерация нормализованного числового датасета norm_num_df(df, prefix = ':1d')\n",
    "from waves_dataset import waves_dataset#Генерация датасета по экстремумам waves_dataset(df, prefix = ':1d')\n",
    "from logic_dataset import logic_dataset#Генерация датасета на основании логических конструкций logic_dataset(df, prefix = ':1d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35337c06",
   "metadata": {},
   "source": [
    "# Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ca8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2d100",
   "metadata": {},
   "source": [
    "# Параметры генерируемого датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaab793",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_params_from_config_file = True #Загрузка параметров из файла\n",
    "load_params_from_command_line = False #Загрузка параметров из командной строки\n",
    "args = None\n",
    "\n",
    "try:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    _ = parser.add_argument('--config_file', dest='config_file', action='store_true', help='Load config from file')\n",
    "    _ = parser.add_argument('--config_path', help='Path to config file: /app/cfg.json')\n",
    "    _ = parser.add_argument('--cmd_config', dest='cmd_config', action='store_true', help='Load config from cmd line')\n",
    "    _ = parser.add_argument('--task_id')\n",
    "    _ = parser.add_argument('--timeframe')\n",
    "    _ = parser.add_argument('--start_date')\n",
    "    _ = parser.add_argument('--end_date')\n",
    "    _ = parser.add_argument('--count_points')\n",
    "    _ = parser.add_argument('--extr_bar_count')\n",
    "    _ = parser.add_argument('--size_df')\n",
    "    _ = parser.add_argument('--max_unmark')\n",
    "    _ = parser.add_argument('--data_path')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if args.config_file:\n",
    "        load_params_from_config_file = True\n",
    "        load_params_from_command_line = False\n",
    "    \n",
    "    if args.cmd_config:\n",
    "            load_params_from_config_file = False\n",
    "            load_params_from_command_line = True\n",
    "except:\n",
    "    print(\"Ошибка парсинга параметров из командной строки\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_params_from_config_file:\n",
    "    #Если есть параметры командной строки\n",
    "    if args:\n",
    "        #Если указан путь к конфигу\n",
    "        if args.config_path:\n",
    "            with open(config_path, 'r', encoding='utf_8') as cfg:\n",
    "                temp_data=cfg.read()\n",
    "        else:\n",
    "            with open('app/configs/1D/data_gen.json', 'r', encoding='utf_8') as cfg:\n",
    "                temp_data=cfg.read()\n",
    "\n",
    "    # parse file\n",
    "    config = json.loads(temp_data)\n",
    "    \n",
    "    task_id = str(config['task_id'])\n",
    "    interval = config['timeframe']\n",
    "    start_date = config['start_date'] #Начальная дата датасета\n",
    "    end_date = config['end_date'] #Конечная дата датасета\n",
    "    count_points = config['count_points'] #Параметр разметки экстремумов\n",
    "    #Сколько размечаем баров начиная с точки экстремума\n",
    "    extr_bar_count = config['extr_bar_count']\n",
    "    #Ограничения размера файла в Гигабайтах\n",
    "    size_df = config['size_df']\n",
    "    #Максимальное количество конечных баров волны в %, которые не размечаем\n",
    "    max_unmark = config['max_unmark']\n",
    "    #Путь для сохранения генерируемых данных\n",
    "    data_path = config['data_path'] #Путь должен быть без чёрточки в конце\n",
    "    \n",
    "if load_params_from_command_line:\n",
    "    task_id = str(args.task_id)\n",
    "    interval = str(args.timeframe)\n",
    "    start_date = str(args.start_date)\n",
    "    end_date = str(args.end_date) \n",
    "    count_points = int(args.count_points)\n",
    "    extr_bar_count = int(args.extr_bar_count) \n",
    "    size_df = int(args.size_df) \n",
    "    max_unmark = float(args.max_unmark) \n",
    "    data_path = str(args.data_path) \n",
    "\n",
    "Y_shift = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9764b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498cbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Смещение категориальных признаков разметки\n",
    "Y_shift = 1\n",
    "\n",
    "#Флаг необходимости формирования трендовых признаков\n",
    "lag_flag = True\n",
    "\n",
    "#Число баров, которые мы кладём в датасет для формирования признаков трендовости\n",
    "#Число включает начальный бар без лага, то есть из 6: 1 - начальный + 5 лаговые\n",
    "#lag_count = 6 #(default)\n",
    "lag_count = 0\n",
    "\n",
    "#Дописывать данные (False) или заново записать датасет (True)\n",
    "new_df = False\n",
    "\n",
    "#Флаг наличия ограничений генерируемых датасетов по размеру\n",
    "size_flag = True\n",
    "\n",
    "#Флаг необходимости удаления не размеченных данных\n",
    "delete_not_marking_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_to_png(graph):\n",
    "    buffer = io.BytesIO()\n",
    "    graph.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    image_png = buffer.getvalue()\n",
    "    buffer.close()\n",
    "    graphic = base64.b64encode(image_png)\n",
    "    graphic = graphic.decode('utf-8')\n",
    "    graph.close()\n",
    "\n",
    "    return graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main (ticker):\n",
    "    \n",
    "    #КОТИРОВКИ!\n",
    "    #Получаем дневные данные\n",
    "#     quotes_1d_temp=yf.Ticker(ticker)\n",
    "#     quotes_1d=quotes_1d_temp.history(\n",
    "#         interval = \"1d\",# valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "#         period=\"max\"\n",
    "#     ) #  1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "#     quotes_1d = quotes_1d.dropna()\n",
    "#     quotes_1d.index = quotes_1d.index.tz_localize(None)\n",
    "#     quotes_1d.sort_index(ascending=True, inplace = True)\n",
    "#     quotes_1d.index.name = \"Datetime\"\n",
    "\n",
    "    # Акции\n",
    "    quotes_temp = Ticker(ticker)\n",
    "    # Свечи по акциям за период\n",
    "    quotes_1d = quotes_temp.candles(date = start_date, till_date = end_date, period=interval)\n",
    "    quotes_1d.head()\n",
    "    \n",
    "    quotes_1d.rename(\n",
    "        columns = {\n",
    "            'begin' : 'Datetime',\n",
    "            'open' : 'Open',\n",
    "            'close' : 'Close',\n",
    "            'high' : 'High',\n",
    "            'low' : 'Low',\n",
    "            'volume' : 'Volume'\n",
    "        }, inplace = True\n",
    "    )\n",
    "    quotes_1d.index = quotes_1d['Datetime']\n",
    "    quotes_1d.sort_index(ascending=True, inplace = True)\n",
    "\n",
    "#     #Фильтруем данные\n",
    "#     if data_filter_flag:\n",
    "#         quotes_1d = date_filter(quotes_1d, filter_data_timezone, filter_data_start, filter_data_end)\n",
    "\n",
    "    #Получаем экстремумы по дневному графику\n",
    "    print('Получаем экстремумы по дневному графику')\n",
    "    quotes_1d_with_extrems = get_extrems(quotes_1d, delete_not_marking_data, count_points = count_points)\n",
    "\n",
    "    #Размечаем Y по дневному графику\n",
    "    quotes_1d_with_Y = quotes_with_Y(quotes_1d_with_extrems, extr_bar_count, Y_shift, max_unmark = max_unmark)\n",
    "\n",
    "    #Очищаем не размеченные данные\n",
    "    quotes_1d_with_Y = quotes_1d_with_Y.dropna(subset = ['Y'])\n",
    "\n",
    "    #Получаем данные индикаторов котировок дневного датафрейма\n",
    "    quotes_1d_indicators = get_indicators(quotes_1d_with_Y, prefix = ':5m')\n",
    "\n",
    "    #Получаем stoch датасет для котировок дневного таймфрейма\n",
    "    stoch_quotes_1d_dataset = get_stoch_indicators(quotes_1d_indicators, prefix = ':5m')\n",
    "\n",
    "    #Получаем датасет логики над стохастиком для котировок дневного таймфрейма\n",
    "    stoch_logic_quotes_1d_dataset = get_stoch_logic_data(stoch_quotes_1d_dataset, prefix = ':5m')\n",
    "    \n",
    "    #Получаем нормализованный числовой датасет для котировок дневного таймфрейма\n",
    "    norm_num_dataset_quotes_1d = norm_num_df(quotes_1d_indicators, prefix = ':5m')\n",
    "\n",
    "    #Свечной анализ\n",
    "    cdl_dataset_quotes_1d = quotes_1d.ta.cdl_pattern(name=\"all\")\n",
    "\n",
    "    #Датасет волн\n",
    "    waves_dataset_quotes_1d =  waves_dataset(quotes_1d_indicators, prefix = ':5m')\n",
    "\n",
    "    #Логический датасет\n",
    "    logic_dataset_quotes_1d =  logic_dataset(quotes_1d_indicators, prefix = ':5m')\n",
    "    \n",
    "    #Собираем датасеты\n",
    "    num_logic_df = pd.DataFrame()\n",
    "    \n",
    "    #Формируем индекс по древным котировкам\n",
    "    num_logic_df.index = quotes_1d.index\n",
    "    \n",
    "    #Инициализируем поля\n",
    "    num_logic_df['Close'] = quotes_1d_with_Y['Close']\n",
    "    num_logic_df['Y'] = quotes_1d_with_Y['Y']\n",
    "    \n",
    "    \n",
    "    #Джойним датасеты\n",
    "    num_logic_df = num_logic_df.join(norm_num_dataset_quotes_1d, lsuffix='_left_num_qout_5m', rsuffix='_right_num_qout_5m')#Нормализованные дневные котировки\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(waves_dataset_quotes_1d, lsuffix='_left_num_qout_5m', rsuffix='_right_num_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(cdl_dataset_quotes_1d, lsuffix='_left_num_qout_5m', rsuffix='_right_num_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(stoch_quotes_1d_dataset, lsuffix='_left_stoch_qout_5m', rsuffix='_right_stoch_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(stoch_logic_quotes_1d_dataset, lsuffix='_left_stoch_qout_5m', rsuffix='_right_stoch_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(logic_dataset_quotes_1d, lsuffix='_left_logic_qout_5m', rsuffix='_right_logic_qout_5m')\n",
    "    \n",
    "    \n",
    "    #Заполняем пустые ячейки предыдущими значениями\n",
    "    num_logic_df = num_logic_df.fillna(method=\"ffill\")\n",
    "     \n",
    "    #Добавляем лаги\n",
    "    #num_df\n",
    "    columns = num_logic_df.columns.values   \n",
    "    for col in columns:\n",
    "        if col not in ['Close', 'Y']:\n",
    "            try:\n",
    "                for i in range(1,lag_count):\n",
    "                    num_logic_df[col+'shift_'+str(i)] = num_logic_df[col].copy(deep = True).shift(i)\n",
    "            except:\n",
    "                #print(\"Ошибка добавления лага в колонке: \", col)\n",
    "                pass\n",
    "    \n",
    "    \n",
    "    #Чистим от пустых значений\n",
    "    num_logic_df = num_logic_df.dropna()\n",
    "    \n",
    "    #Конвертируем индексы\n",
    "    num_logic_df.index = num_logic_df.index.astype(int)\n",
    "    \n",
    "    #Разбиваем датасеты\n",
    "    num_logic_df_train, num_logic_df_test = train_test_split(num_logic_df, test_size=0.1, shuffle=False)\n",
    "    \n",
    "    #Записываем датасеты\n",
    "    print(\"Записываем датасеты: \", ticker)\n",
    "    #Проверяем на существование Если не существуют то записываем первый раз с заголовком\n",
    "    #Если существуют до дописываем без заголовка\n",
    "    if not os.path.exists(data_path+\"/num_logic_1d_1w_train.csv\"):\n",
    "        num_logic_df_train.to_csv(data_path+\"/num_logic_1d_1w_train.csv\")\n",
    "    else:\n",
    "        num_logic_df_train.to_csv(data_path+\"/num_logic_1d_1w_train.csv\", mode='a', header= False)\n",
    "    \n",
    "    if not os.path.exists(\"app/data/num_logic_1d_1w_test.csv\"):\n",
    "        num_logic_df_test.to_csv(data_path+\"/num_logic_1d_1w_test.csv\")\n",
    "    else:\n",
    "        num_logic_df_test.to_csv(data_path+\"/num_logic_1d_1w_test.csv\", mode='a', header= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37df652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main('SBER')\n",
    "#main('ABIO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754882e7",
   "metadata": {},
   "source": [
    "# Проверяем контрольную точку продолжения генерации датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52648857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_size():\n",
    "    #Проверяем наличие датасетов\n",
    "    folder = Path(data_path)\n",
    "    if os.path.exists(data_path):\n",
    "        if sum(1 for x in folder.iterdir()) > 0:\n",
    "            #Проверяем ограничения на размер файла\n",
    "            if size_flag:\n",
    "                size_arr = []\n",
    "                try:\n",
    "                    size_arr.append(os.path.getsize(data_path+\"/num_logic_1d_1w_train.csv\")/(1024*1024*1024))\n",
    "                except:\n",
    "                    size_arr.append(0)\n",
    "\n",
    "                max_size_df = max(size_arr)\n",
    "\n",
    "                if max_size_df > size_df:\n",
    "                    print (\"Достигнут предел по размеру датасетов\")\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "        else:\n",
    "            return True\n",
    "            \n",
    "    else:\n",
    "        os.mkdir(data_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53f879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверяем наличие датасетов\n",
    "folder = Path(data_path)\n",
    "if os.path.exists(data_path):\n",
    "    if sum(1 for x in folder.iterdir()) > 0:\n",
    "        #Проверяем ограничения на размер файла\n",
    "        if size_flag:\n",
    "            size_arr = []\n",
    "            try:\n",
    "                size_arr.append(os.path.getsize(data_path+\"/num_logic_1d_1w_train.csv\")/(1024*1024*1024))\n",
    "            except:\n",
    "                size_arr.append(0)\n",
    "            \n",
    "            max_size_df = max(size_arr)\n",
    "            \n",
    "            if max_size_df > size_df:\n",
    "                list_flag = False\n",
    "                print (\"Достигнут предел по размеру датасетов\")\n",
    "        \n",
    "        #Пытаемся загрузить последний тикер генерации датасета\n",
    "        try:\n",
    "            with open('save/last_ticker.pickle', 'rb') as f:\n",
    "                last_ticker = pickle.load(f)\n",
    "        except:\n",
    "            print(\"Отсутствуют данные сохранения\")\n",
    "else:\n",
    "    os.mkdir(\"app/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8be67",
   "metadata": {},
   "source": [
    "# Загружаем список для генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = Market('stocks')\n",
    "stocks\n",
    "\n",
    "#tickers_list = stocks.tradestats(date='2023-10-10')['ticker'].values\n",
    "tickers_list = stocks.tradestats(date='2023-10-10').groupby('ticker').agg(\n",
    "    {\n",
    "        'val': 'sum'\n",
    "    }\n",
    ").sort_values(by = ['val'], ascending = False).reset_index()['ticker'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588ab3f",
   "metadata": {},
   "source": [
    "# Генерируем датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711519d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(tickers_list) > 0:\n",
    "    for ticker in tickers_list:\n",
    "\n",
    "        print(ticker)\n",
    "        if check_size():\n",
    "\n",
    "            #Генерируем датасет\n",
    "            print(\"Начало обработки нового тикера: \", ticker)\n",
    "            try:\n",
    "                main(ticker)\n",
    "            except:\n",
    "                print(\"Ошибка генерации датасета: \", ticker)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "else:\n",
    "    print(\"Список для обработки пуст\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c69c13",
   "metadata": {},
   "source": [
    "# Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Соединение с БД\n",
    "def connect():\n",
    "    return psycopg2.connect(\n",
    "        host=global_config.db_host,\n",
    "        database=global_config.db_database,\n",
    "        user=global_config.db_user,\n",
    "        password=global_config.db_password\n",
    "    )\n",
    "conn = connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обновляем данные по задаче\n",
    "if conn.closed == 1:\n",
    "    conn = connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql = \"\"\" UPDATE public.data_gen_tasks\n",
    "            SET task_status = %s\n",
    "            WHERE id = %s\"\"\"\n",
    "try:\n",
    "    cur.execute(sql, ('done', task_id))\n",
    "except:\n",
    "    print(\"Ошибка записи информации о закрытии задачи в БД\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcf0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1559e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0482c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2587d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7bdbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef512e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3f864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e81ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6208e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fc4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8640071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
